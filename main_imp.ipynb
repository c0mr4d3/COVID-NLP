{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-18e440bc17cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0msearcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpysearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimpleSearcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mluceneDir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mhits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mn_hits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m## collect the relevant data in a hit dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query' is not defined"
     ]
    }
   ],
   "source": [
    "from pyserini.search import pysearch\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "USE_SUMMARY = True\n",
    "FIND_PDFS = True\n",
    "SEARCH_MEDRXIV = True\n",
    "SEARCH_PUBMED = True\n",
    "\n",
    "import torch\n",
    "luceneDir = 'lucene-index-covid-2020-04-10/'\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "QA_MODEL = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "QA_TOKENIZER = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "QA_MODEL.to(torch_device)\n",
    "QA_MODEL.eval()\n",
    "\n",
    "if USE_SUMMARY:\n",
    "    SUMMARY_TOKENIZER = BartTokenizer.from_pretrained('bart-large-cnn')\n",
    "    SUMMARY_MODEL = BartForConditionalGeneration.from_pretrained('bart-large-cnn')\n",
    "    SUMMARY_MODEL.to(torch_device)\n",
    "    SUMMARY_MODEL.eval()\n",
    "\n",
    "from Bio import Entrez, Medline\n",
    "\n",
    "try:\n",
    "    from StringIO import StringIO\n",
    "except ImportError:\n",
    "    from io import StringIO\n",
    "import re\n",
    "\n",
    "import json\n",
    "searcher = pysearch.SimpleSearcher(luceneDir)\n",
    "hits = searcher.search(query + '. ' + keywords)\n",
    "n_hits = len(hits)\n",
    "## collect the relevant data in a hit dictionary\n",
    "hit_dictionary = {}\n",
    "for i in range(0, n_hits):\n",
    "    doc_json = json.loads(hits[i].raw)\n",
    "    idx = str(hits[i].docid)\n",
    "    hit_dictionary[idx] = doc_json\n",
    "    hit_dictionary[idx]['title'] = hits[i].lucene_document.get(\"title\")\n",
    "    hit_dictionary[idx]['authors'] = hits[i].lucene_document.get(\"authors\")\n",
    "    hit_dictionary[idx]['doi'] = hits[i].lucene_document.get(\"doi\")\n",
    "\n",
    "\n",
    "## scrub the abstracts in prep for BERT-SQuAD\n",
    "for idx,v in hit_dictionary.items():\n",
    "    abs_dirty = v['abstract']\n",
    "    # looks like the abstract value can be an empty list\n",
    "    v['abstract_paragraphs'] = []\n",
    "    v['abstract_full'] = ''\n",
    "\n",
    "    if abs_dirty:\n",
    "        # looks like if it is a list, then the only entry is a dictionary wher text is in 'text' key\n",
    "        # looks like it is broken up by paragraph if it is in that form.  lets make lists for every paragraph\n",
    "        # and a new entry that is full abstract text as both could be valuable for BERT derrived QA\n",
    "\n",
    "\n",
    "        if isinstance(abs_dirty, list):\n",
    "            for p in abs_dirty:\n",
    "                v['abstract_paragraphs'].append(p['text'])\n",
    "                v['abstract_full'] += p['text'] + ' \\n\\n'\n",
    "\n",
    "        # looks like in some cases the abstract can be straight up text so we can actually leave that alone\n",
    "        if isinstance(abs_dirty, str):\n",
    "            v['abstract_paragraphs'].append(abs_dirty)\n",
    "            v['abstract_full'] += abs_dirty + ' \\n\\n'\n",
    "\n",
    "def embed_useT(module):\n",
    "    with tf.Graph().as_default():\n",
    "        sentences = tf.compat.v1.placeholder(tf.string)\n",
    "        embed = hub.Module(module)\n",
    "        embeddings = embed(sentences)\n",
    "        session = tf.compat.v1.train.MonitoredSession()\n",
    "    return lambda x: session.run(embeddings, {sentences: x})\n",
    "\n",
    "embed_fn = embed_useT('kaggle/working/sentence_wise_email/module/module_useT')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "def reconstructText(tokens, start=0, stop=-1):\n",
    "    tokens = tokens[start: stop]\n",
    "    if '[SEP]' in tokens:\n",
    "        sepind = tokens.index('[SEP]')\n",
    "        tokens = tokens[sepind+1:]\n",
    "    txt = ' '.join(tokens)\n",
    "    txt = txt.replace(' ##', '')\n",
    "    txt = txt.replace('##', '')\n",
    "    txt = txt.strip()\n",
    "    txt = \" \".join(txt.split())\n",
    "    txt = txt.replace(' .', '.')\n",
    "    txt = txt.replace('( ', '(')\n",
    "    txt = txt.replace(' )', ')')\n",
    "    txt = txt.replace(' - ', '-')\n",
    "    txt_list = txt.split(' , ')\n",
    "    txt = ''\n",
    "    nTxtL = len(txt_list)\n",
    "    if nTxtL == 1:\n",
    "        return txt_list[0]\n",
    "    newList =[]\n",
    "    for i,t in enumerate(txt_list):\n",
    "        if i < nTxtL -1:\n",
    "            if t[-1].isdigit() and txt_list[i+1][0].isdigit():\n",
    "                newList += [t,',']\n",
    "            else:\n",
    "                newList += [t, ', ']\n",
    "        else:\n",
    "            newList += [t]\n",
    "    return ''.join(newList)\n",
    "\n",
    "\n",
    "def makeBERTSQuADPrediction(document, question):\n",
    "    ## we need to rewrite this function so that it chuncks the document into 250-300 word segments with\n",
    "    ## 50 word overlaps on either end so that it can understand and check longer abstracts\n",
    "    nWords = len(document.split())\n",
    "    input_ids_all = QA_TOKENIZER.encode(question, document)\n",
    "    tokens_all = QA_TOKENIZER.convert_ids_to_tokens(input_ids_all)\n",
    "    overlapFac = 1.1\n",
    "    if len(input_ids_all)*overlapFac > 2048:\n",
    "        nSearchWords = int(np.ceil(nWords/5))\n",
    "        quarter = int(np.ceil(nWords/4))\n",
    "        docSplit = document.split()\n",
    "        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
    "                     ' '.join(docSplit[quarter-int(nSearchWords*overlapFac/2):quarter+int(quarter*overlapFac/2)]),\n",
    "                     ' '.join(docSplit[quarter*2-int(nSearchWords*overlapFac/2):quarter*2+int(quarter*overlapFac/2)]),\n",
    "                     ' '.join(docSplit[quarter*3-int(nSearchWords*overlapFac/2):quarter*3+int(quarter*overlapFac/2)]),\n",
    "                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]        \n",
    "        \n",
    "    elif len(input_ids_all)*overlapFac > 1536:\n",
    "        nSearchWords = int(np.ceil(nWords/4))\n",
    "        third = int(np.ceil(nWords/3))\n",
    "        docSplit = document.split()\n",
    "        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
    "                     ' '.join(docSplit[third-int(nSearchWords*overlapFac/2):third+int(nSearchWords*overlapFac/2)]),\n",
    "                     ' '.join(docSplit[third*2-int(nSearchWords*overlapFac/2):third*2+int(nSearchWords*overlapFac/2)]),\n",
    "                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]        \n",
    "        \n",
    "    elif len(input_ids_all)*overlapFac > 1024:\n",
    "        nSearchWords = int(np.ceil(nWords/3))\n",
    "        middle = int(np.ceil(nWords/2))\n",
    "        docSplit = document.split()\n",
    "        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
    "                     ' '.join(docSplit[middle-int(nSearchWords*overlapFac/2):middle+int(nSearchWords*overlapFac/2)]),\n",
    "                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
    "    elif len(input_ids_all)*overlapFac > 512:\n",
    "        nSearchWords = int(np.ceil(nWords/2))\n",
    "        docSplit = document.split()\n",
    "        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
    "    else:\n",
    "        input_ids = [input_ids_all]\n",
    "    absTooLong = False    \n",
    "    \n",
    "    answers = []\n",
    "    cons = []\n",
    "    for iptIds in input_ids:\n",
    "        tokens = QA_TOKENIZER.convert_ids_to_tokens(iptIds)\n",
    "        sep_index = iptIds.index(QA_TOKENIZER.sep_token_id)\n",
    "        num_seg_a = sep_index + 1\n",
    "        num_seg_b = len(iptIds) - num_seg_a\n",
    "        segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "        assert len(segment_ids) == len(iptIds)\n",
    "        n_ids = len(segment_ids)\n",
    "        #print(n_ids)\n",
    "\n",
    "        if n_ids < 512:\n",
    "            start_scores, end_scores = QA_MODEL(torch.tensor([iptIds]).to(torch_device), \n",
    "                                     token_type_ids=torch.tensor([segment_ids]).to(torch_device))\n",
    "        else:\n",
    "            #this cuts off the text if its more than 512 words so it fits in model space\n",
    "            #need run multiple inferences for longer text. add to the todo\n",
    "            print('****** warning only considering first 512 tokens, document is '+str(nWords)+' words long.  There are '+str(n_ids)+ ' tokens')\n",
    "            absTooLong = True\n",
    "            start_scores, end_scores = QA_MODEL(torch.tensor([iptIds[:512]]).to(torch_device), \n",
    "                                     token_type_ids=torch.tensor([segment_ids[:512]]).to(torch_device))\n",
    "        start_scores = start_scores[:,1:-1]\n",
    "        end_scores = end_scores[:,1:-1]\n",
    "        answer_start = torch.argmax(start_scores)\n",
    "        answer_end = torch.argmax(end_scores)\n",
    "        #print(answer_start, answer_end)\n",
    "        answer = reconstructText(tokens, answer_start, answer_end+2)\n",
    "    \n",
    "        if answer.startswith('. ') or answer.startswith(', '):\n",
    "            answer = answer[2:]\n",
    "            \n",
    "        c = start_scores[0,answer_start].item()+end_scores[0,answer_end].item()\n",
    "        answers.append(answer)\n",
    "        cons.append(c)\n",
    "    \n",
    "    maxC = max(cons)\n",
    "    iMaxC = [i for i, j in enumerate(cons) if j == maxC][0]\n",
    "    confidence = cons[iMaxC]\n",
    "    answer = answers[iMaxC]\n",
    "    \n",
    "    sep_index = tokens_all.index('[SEP]')\n",
    "    full_txt_tokens = tokens_all[sep_index+1:]\n",
    "    \n",
    "    abs_returned = reconstructText(full_txt_tokens)\n",
    "\n",
    "    ans={}\n",
    "    ans['answer'] = answer\n",
    "    #print(answer)\n",
    "    if answer.startswith('[CLS]') or answer_end.item() < sep_index or answer.endswith('[SEP]'):\n",
    "        ans['confidence'] = -1000000\n",
    "    else:\n",
    "        #confidence = torch.max(start_scores) + torch.max(end_scores)\n",
    "        #confidence = np.log(confidence.item())\n",
    "        ans['confidence'] = confidence\n",
    "    #ans['start'] = answer_start.item()\n",
    "    #ans['end'] = answer_end.item()\n",
    "    ans['abstract_bert'] = abs_returned\n",
    "    ans['abs_too_long'] = absTooLong\n",
    "    return ans\n",
    "\n",
    "from tqdm import tqdm\n",
    "def searchAbstracts(hit_dictionary, question):\n",
    "    abstractResults = {}\n",
    "    for k,v in tqdm(hit_dictionary.items()):\n",
    "        abstract = v['abstract_full']\n",
    "        if abstract:\n",
    "            ans = makeBERTSQuADPrediction(abstract, question)\n",
    "            if ans['answer']:\n",
    "                confidence = ans['confidence']\n",
    "                abstractResults[confidence]={}\n",
    "                abstractResults[confidence]['answer'] = ans['answer']\n",
    "                #abstractResults[confidence]['start'] = ans['start']\n",
    "                #abstractResults[confidence]['end'] = ans['end']\n",
    "                abstractResults[confidence]['abstract_bert'] = ans['abstract_bert']\n",
    "                abstractResults[confidence]['idx'] = k\n",
    "                abstractResults[confidence]['abs_too_long'] = ans['abs_too_long']\n",
    "                \n",
    "    cList = list(abstractResults.keys())\n",
    "\n",
    "    if cList:\n",
    "        maxScore = max(cList)\n",
    "        total = 0.0\n",
    "        exp_scores = []\n",
    "        for c in cList:\n",
    "            s = np.exp(c-maxScore)\n",
    "            exp_scores.append(s)\n",
    "        total = sum(exp_scores)\n",
    "        for i,c in enumerate(cList):\n",
    "            abstractResults[exp_scores[i]/total] = abstractResults.pop(c)\n",
    "    return abstractResults\n",
    "\n",
    "answers = searchAbstracts(hit_dictionary, query)\n",
    "\n",
    "workingPath = '/kaggle/working'\n",
    "import pandas as pd\n",
    "if FIND_PDFS:\n",
    "    from metapub import UrlReverse\n",
    "    from metapub import FindIt\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "#from summarizer import Summarizer\n",
    "#summarizerModel = Summarizer()\n",
    "def displayResults(hit_dictionary, answers, question):\n",
    "    \n",
    "    question_HTML = '<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query</b>: '+question+'</div>'\n",
    "    #all_HTML_txt = question_HTML\n",
    "    confidence = list(answers.keys())\n",
    "    confidence.sort(reverse=True)\n",
    "    \n",
    "    confidence = list(answers.keys())\n",
    "    confidence.sort(reverse=True)\n",
    "    \n",
    "\n",
    "    for c in confidence:\n",
    "        if c>0 and c <= 1 and len(answers[c]['answer']) != 0:\n",
    "            if 'idx' not in  answers[c]:\n",
    "                continue\n",
    "            rowData = []\n",
    "            idx = answers[c]['idx']\n",
    "            title = hit_dictionary[idx]['title']\n",
    "            authors = hit_dictionary[idx]['authors'] + ' et al.'\n",
    "            doi = '<a href=\"https://doi.org/'+hit_dictionary[idx]['doi']+'\" target=\"_blank\">' + title +'</a>'\n",
    "\n",
    "            \n",
    "            full_abs = answers[c]['abstract_bert']\n",
    "            bert_ans = answers[c]['answer']\n",
    "            \n",
    "            \n",
    "            split_abs = full_abs.split(bert_ans)\n",
    "            sentance_beginning = split_abs[0][split_abs[0].rfind('.')+1:]\n",
    "            if len(split_abs) == 1:\n",
    "                sentance_end_pos = len(full_abs)\n",
    "                sentance_end =''\n",
    "            else:\n",
    "                sentance_end_pos = split_abs[1].find('. ')+1\n",
    "                if sentance_end_pos == 0:\n",
    "                    sentance_end = split_abs[1]\n",
    "                else:\n",
    "                    sentance_end = split_abs[1][:sentance_end_pos]\n",
    "                \n",
    "            #sentance_full = sentance_beginning + bert_ans+ sentance_end\n",
    "            answers[c]['full_answer'] = sentance_beginning+bert_ans+sentance_end\n",
    "            answers[c]['sentence_beginning'] = sentance_beginning\n",
    "            answers[c]['sentence_end'] = sentance_end\n",
    "            answers[c]['title'] = title\n",
    "            answers[c]['doi'] = doi\n",
    "            if 'pdfLink' in hit_dictionary[idx]:\n",
    "                answers[c]['pdfLink'] = hit_dictionary[idx]['pdfLink']\n",
    "                \n",
    "        else:\n",
    "            answers.pop(c)\n",
    "    \n",
    "    \n",
    "    ## now rerank based on semantic similarity of the answers to the question\n",
    "    cList = list(answers.keys())\n",
    "    allAnswers = [answers[c]['full_answer'] for c in cList]\n",
    "    \n",
    "    messages = [question]+allAnswers\n",
    "    \n",
    "    encoding_matrix = embed_fn(messages)\n",
    "    similarity_matrix = np.inner(encoding_matrix, encoding_matrix)\n",
    "    rankings = similarity_matrix[1:,0]\n",
    "    \n",
    "    for i,c in enumerate(cList):\n",
    "        answers[rankings[i]] = answers.pop(c)\n",
    "\n",
    "    ## now form pandas dv\n",
    "    confidence = list(answers.keys())\n",
    "    confidence.sort(reverse=True)\n",
    "    pandasData = []\n",
    "    ranked_aswers = []\n",
    "    for c in confidence:\n",
    "        rowData=[]\n",
    "        title = answers[c]['title']\n",
    "        doi = answers[c]['doi']\n",
    "        idx = answers[c]['idx']\n",
    "        rowData += [idx]            \n",
    "        sentance_html = '<div>' +answers[c]['sentence_beginning'] + \" <font color='red'>\"+answers[c]['answer']+\"</font> \"+answers[c]['sentence_end']+'</div>'\n",
    "        \n",
    "        rowData += [sentance_html, c, doi]\n",
    "        pandasData.append(rowData)\n",
    "        ranked_aswers.append(' '.join([answers[c]['full_answer']]))\n",
    "    \n",
    "    if FIND_PDFS or SEARCH_MEDRXIV:\n",
    "        pdata2 = []\n",
    "        for rowData in pandasData:\n",
    "            rd = rowData\n",
    "            idx = rowData[0]\n",
    "            if 'pdfLink' in answers[rowData[2]]:\n",
    "                rd += ['<a href=\"'+answers[rowData[2]]['pdfLink']+'\" target=\"_blank\">PDF Link</a>']\n",
    "            elif FIND_PDFS:\n",
    "                if str(idx).startswith('pm_'):\n",
    "                    pmid = idx[3:]\n",
    "                else:\n",
    "                    try:\n",
    "                        test = UrlReverse('https://doi.org/'+hit_dictionary[idx]['doi'])\n",
    "                        if test is not None:\n",
    "                            pmid = test.pmid\n",
    "                        else:\n",
    "                            pmid = None\n",
    "                    except:\n",
    "                        pmid = None\n",
    "                pdfLink = None\n",
    "                if pmid is not None:\n",
    "                    try:\n",
    "                        pdfLink = FindIt(str(pmid))\n",
    "                    except:\n",
    "                        pdfLink = None\n",
    "                if pdfLink is not None:\n",
    "                    pdfLink = pdfLink.url\n",
    "\n",
    "                if pdfLink is None:\n",
    "\n",
    "                    rd += ['Not Available']\n",
    "                else:\n",
    "                    rd += ['<a href=\"'+pdfLink+'\" target=\"_blank\">PDF Link</a>']\n",
    "            else:\n",
    "                rd += ['Not Available']\n",
    "            pdata2.append(rowData)\n",
    "    else:\n",
    "        pdata2 = pandasData\n",
    "        \n",
    "    \n",
    "    display(HTML(question_HTML))\n",
    "    \n",
    "    if USE_SUMMARY:\n",
    "        ## try generating an exacutive summary with extractive summarizer\n",
    "        allAnswersTxt = ' '.join(ranked_aswers[:6]).replace('\\n','')\n",
    "    #    exec_sum = summarizerModel(allAnswersTxt, min_length=1, max_length=500)    \n",
    "     #   execSum_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:18px\"><b>BERT Extractive Summary:</b>: '+exec_sum+'</div>'\n",
    "\n",
    "        answers_input_ids = SUMMARY_TOKENIZER.batch_encode_plus([allAnswersTxt], return_tensors='pt', max_length=1024)['input_ids'].to(torch_device)\n",
    "        summary_ids = SUMMARY_MODEL.generate(answers_input_ids,\n",
    "                                               num_beams=10,\n",
    "                                               length_penalty=1.2,\n",
    "                                               max_length=1024,\n",
    "                                               min_length=64,\n",
    "                                               no_repeat_ngram_size=4)\n",
    "\n",
    "        exec_sum = SUMMARY_TOKENIZER.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
    "        execSum_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; margin-bottom:1pt\"><b>BART Abstractive Summary:</b>: '+exec_sum+'</div>'\n",
    "        display(HTML(execSum_HTML))\n",
    "        warning_HTML = '<div style=\"font-family: Times New Roman; font-size: 12px; padding-bottom:12px; color:#CCCC00; margin-top:1pt\"> Warning this is an autogenerated summary based on semantic search of abstracts, always examine the sources before accepting this conclusion.  If the evidence only mentions topic in passing or the evidence is not clear, the summary will likely not clearly answer the question.</div>'\n",
    "        display(HTML(warning_HTML))\n",
    "\n",
    "#    display(HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:18px\"><b>Body of Evidence:</b></div>'))\n",
    "    \n",
    "    if FIND_PDFS or SEARCH_MEDRXIV:\n",
    "        df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Title/Link','PDF Link'])\n",
    "    else:\n",
    "        df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Title/Link'])\n",
    "        \n",
    "    display(HTML(df.to_html(render_links=True, escape=False)))\n",
    "    \n",
    "#displayResults(hit_dictionary, answers, query)\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "import dateutil.parser as dparser\n",
    "\n",
    "def medrxivSearch(query, n_pages=1):\n",
    "    results = {}\n",
    "    q = query\n",
    "    for x in range(n_pages):\n",
    "        PARAMS = {\n",
    "            'page': x\n",
    "        }\n",
    "        r = requests.get('https://www.medrxiv.org/search/' + q, params = PARAMS)\n",
    "        content = r.text\n",
    "        page = BeautifulSoup(content, 'lxml')\n",
    "        \n",
    "        for entry in page.find_all(\"a\", attrs={\"class\": \"highwire-cite-linked-title\"}):\n",
    "            title = \"\"\n",
    "            url = \"\"\n",
    "            pubDate = \"\"\n",
    "            journal = None\n",
    "            abstract = \"\"\n",
    "            authors = []\n",
    "            database = \"medRxiv\"\n",
    "            \n",
    "            \n",
    "            url = \"https://www.medrxiv.org\" + entry.get('href')\n",
    "            \n",
    "            request_entry = requests.get(url)\n",
    "            content_entry = request_entry.text\n",
    "            page_entry = BeautifulSoup(content_entry, 'lxml')\n",
    "            doi = page_entry.find(\"span\", attrs={\"class\": \"highwire-cite-metadata-doi\"}).text.split('doi.org/')[-1]\n",
    "            #print(page_entry)\n",
    "\n",
    "            #getting pubDate\n",
    "            pubDate = page_entry.find_all(\"div\", attrs = {\"class\": \"pane-content\"})\n",
    "            pubDate = pubDate[10]\n",
    "            pubDate = str(dparser.parse(pubDate, fuzzy = True))\n",
    "            pubDate = datetime.datetime.strptime(pubDate, '%Y-%m-%d %H:%M:%S')\n",
    "            pubDate = pubDate.strftime('%b %d %Y')\n",
    "            date = pubDate.split()\n",
    "            month = date[0]\n",
    "            day = date[1]\n",
    "            year = date[2]\n",
    "            pubDate = {\n",
    "                'year': year,\n",
    "                'month': month,\n",
    "                'day': day\n",
    "            }\n",
    "\n",
    "            #getting title\n",
    "            title = page_entry.find(\"h1\", attrs={\"class\": \"highwire-cite-title\"}).text\n",
    "            \n",
    "            #getting abstract\n",
    "            abstract = page_entry.find(\"p\", attrs = {\"id\": \"p-2\"}).text.replace('\\n', ' ')\n",
    "\n",
    "            #getting authors \n",
    "            givenNames = page_entry.find_all(\"span\", attrs={\"class\": \"nlm-given-names\"})\n",
    "            surnames = page_entry.find_all(\"span\",  attrs={\"class\": \"nlm-surname\"})\n",
    "            names = list(zip(givenNames,surnames))\n",
    "            for author in names:\n",
    "                name = author[0].text + ' ' + author[1].text\n",
    "                if name not in authors:\n",
    "                    authors.append(name)\n",
    "            \n",
    "            result = {\n",
    "                'title': title,\n",
    "                'url': url,\n",
    "                'pubDate': pubDate,\n",
    "                'journal': journal,\n",
    "                'abstract': abstract,\n",
    "                'authors': authors[0],\n",
    "                'database': database,\n",
    "                'doi': doi,\n",
    "                'pdfLink': url+'.full.pdf'\n",
    "            }\n",
    "            results['mrx_'+result['doi'].split('/')[-1]] = result\n",
    "            #break\n",
    "    return results\n",
    "\n",
    "def searchDatabase(question, keywords, pysearch, lucene_database, pm_kw = '', minDate='2019/12/01', k=20):\n",
    "    ## search the lucene database with a combination of the question and the keywords\n",
    "    searcher = pysearch.SimpleSearcher(lucene_database)\n",
    "    hits = searcher.search(question + '. ' + keywords, k=k)\n",
    "    n_hits = len(hits)\n",
    "    ## collect the relevant data in a hit dictionary\n",
    "    hit_dictionary = {}\n",
    "    for i in range(0, n_hits):\n",
    "        doc_json = json.loads(hits[i].raw)\n",
    "        idx = str(hits[i].docid)\n",
    "        hit_dictionary[idx] = doc_json\n",
    "        hit_dictionary[idx]['title'] = hits[i].lucene_document.get(\"title\")\n",
    "        hit_dictionary[idx]['authors'] = hits[i].lucene_document.get(\"authors\")\n",
    "        hit_dictionary[idx]['doi'] = hits[i].lucene_document.get(\"doi\")\n",
    "        \n",
    "    \n",
    "    titleList = [h['title'] for h in hit_dictionary.values()]\n",
    "    \n",
    "    if pm_kw:\n",
    "        if SEARCH_PUBMED:\n",
    "            new_hits = pubMedSearch(pm_kw, db='pubmed', mindate=minDate)\n",
    "            for id,h in new_hits.items():\n",
    "                if h['title'] not in titleList:\n",
    "                    titleList.append(h['title'])\n",
    "                hit_dictionary[id] = h\n",
    "        if SEARCH_MEDRXIV:\n",
    "            new_hits = medrxivSearch(pm_kw)\n",
    "            for id,h in new_hits.items():\n",
    "                if h['title'] not in titleList:\n",
    "                    titleList.append(h['title'])\n",
    "                hit_dictionary[id] = h\n",
    "    \n",
    "    ## scrub the abstracts in prep for BERT-SQuAD\n",
    "    for idx,v in hit_dictionary.items():\n",
    "        abs_dirty = v['abstract']\n",
    "        # looks like the abstract value can be an empty list\n",
    "        v['abstract_paragraphs'] = []\n",
    "        v['abstract_full'] = ''\n",
    "\n",
    "        if abs_dirty:\n",
    "            # looks like if it is a list, then the only entry is a dictionary wher text is in 'text' key\n",
    "            # looks like it is broken up by paragraph if it is in that form.  lets make lists for every paragraph\n",
    "            # and a new entry that is full abstract text as both could be valuable for BERT derrived QA\n",
    "\n",
    "\n",
    "            if isinstance(abs_dirty, list):\n",
    "                for p in abs_dirty:\n",
    "                    v['abstract_paragraphs'].append(p['text'])\n",
    "                    v['abstract_full'] += p['text'] + ' \\n\\n'\n",
    "\n",
    "            # looks like in some cases the abstract can be straight up text so we can actually leave that alone\n",
    "            if isinstance(abs_dirty, str):\n",
    "                v['abstract_paragraphs'].append(abs_dirty)\n",
    "                v['abstract_full'] += abs_dirty + ' \\n\\n'\n",
    "    ## Search collected abstracts with BERT-SQuAD\n",
    "    answers = searchAbstracts(hit_dictionary, question)\n",
    "    \n",
    "    ## display results in a nice format\n",
    "    displayResults(hit_dictionary, answers, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('DeepLearning': conda)",
   "language": "python",
   "name": "python37764bitdeeplearningconda0375255256d54e4184934ecef3e5f689"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
